{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3631d105-65d1-4df6-9d8d-6c848e1cebf1",
   "metadata": {},
   "source": [
    "# Project2 : Analyse des Maladies de l’Arachide à l’Aide d’algorithmes d’intelligence artificielle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cc818-751d-476d-94fa-3dbd31229138",
   "metadata": {},
   "source": [
    "* Abdou Aziz Kane\n",
    "* Maguette Leye\n",
    "* Aboubacar Sadikh Sow\n",
    "* Abdoukhadre Sylla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5623fb5-118b-44d3-9946-669d1bcf1d5c",
   "metadata": {},
   "source": [
    "### Importation des bibliothèques nécessaires\n",
    "* Cette section importe les modules nécessaires pour la manipulation des données, la création de modèles et l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d4ca0c-d9ab-4087-a436-4825c2e646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1cd0a-e82e-42b3-a819-02137d03c02d",
   "metadata": {},
   "source": [
    "### Définition du chemin des données\n",
    "* Définit le chemin où sont stockées les images pour l'entraînement et la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb981d67-8417-4c91-9f6b-852fd092d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\abous\\Desktop\\DATASET\\train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70702c18-d395-4aeb-902b-761accdfa6a4",
   "metadata": {},
   "source": [
    "### Définition des transformations des données\n",
    "* Applique des transformations pour augmenter, redimensionner et normaliser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f80a7c-8c9e-4552-b568-849c74f389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efd03e-b7c5-41dd-93ed-dfd952fbb30a",
   "metadata": {},
   "source": [
    "### Chargement et division des données\n",
    "* Charge les images depuis le dossier et les divise en ensembles d'entraînement, validation et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef5766c-9cc9-4a50-b231-f6221dcd0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837bdb3-478b-4d0f-9ff5-8ef379c98623",
   "metadata": {},
   "source": [
    "### Définition du modèle CNN amélioré\n",
    "* Déclare une classe CNN améliorée pour la classification des maladies des feuilles d'arachide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87289c3a-94b3-4cd7-a129-751d1856045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):  # Correction de __init__\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fadbb-0941-45b7-9ede-7967b95d946e",
   "metadata": {},
   "source": [
    "### Initialisation du modèle et des paramètres d'entraînement\n",
    "* Crée une instance du modèle et définit le périphérique d'exécution (GPU ou CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c70010-6fcc-47e5-b42d-731b79bae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.classes)\n",
    "model = ImprovedCNN(num_classes=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72675601-9414-4369-b471-d10dd9c19400",
   "metadata": {},
   "source": [
    "### Définition de la fonction d'entraînement\n",
    "* Entraîne le modèle, affiche la perte et sauvegarde le meilleur modèle en fonction de la précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13baed39-6395-4345-bb96-e90f60cfba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9ab7a-b205-4a2e-b490-9c7a49358271",
   "metadata": {},
   "source": [
    "### Exécution de l'entraînement du modèle\n",
    "* Lance l'entraînement du modèle et affiche la meilleure précision atteinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2db7771-77f7-43ee-8b5f-a251c8d62b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7797\n",
      "Validation Accuracy: 39.44%\n",
      "Epoch 2, Loss: 1.4463\n",
      "Validation Accuracy: 34.64%\n",
      "Epoch 3, Loss: 1.3558\n",
      "Validation Accuracy: 42.86%\n",
      "Epoch 4, Loss: 1.2888\n",
      "Validation Accuracy: 50.70%\n",
      "Epoch 5, Loss: 1.2263\n",
      "Validation Accuracy: 53.73%\n",
      "Epoch 6, Loss: 1.1198\n",
      "Validation Accuracy: 63.59%\n",
      "Epoch 7, Loss: 1.0383\n",
      "Validation Accuracy: 58.41%\n",
      "Epoch 8, Loss: 0.9708\n",
      "Validation Accuracy: 64.48%\n",
      "Epoch 9, Loss: 0.8689\n",
      "Validation Accuracy: 71.93%\n",
      "Epoch 10, Loss: 0.8098\n",
      "Validation Accuracy: 70.67%\n",
      "Epoch 11, Loss: 0.7385\n",
      "Validation Accuracy: 65.99%\n",
      "Epoch 12, Loss: 0.7030\n",
      "Validation Accuracy: 74.34%\n",
      "Epoch 13, Loss: 0.6518\n",
      "Validation Accuracy: 80.15%\n",
      "Epoch 14, Loss: 0.5874\n",
      "Validation Accuracy: 82.68%\n",
      "Epoch 15, Loss: 0.5761\n",
      "Validation Accuracy: 81.54%\n",
      "Epoch 16, Loss: 0.5550\n",
      "Validation Accuracy: 83.06%\n",
      "Epoch 17, Loss: 0.5249\n",
      "Validation Accuracy: 82.43%\n",
      "Epoch 18, Loss: 0.5159\n",
      "Validation Accuracy: 82.93%\n",
      "Epoch 19, Loss: 0.4978\n",
      "Validation Accuracy: 86.09%\n",
      "Epoch 20, Loss: 0.4932\n",
      "Validation Accuracy: 83.82%\n",
      "Epoch 21, Loss: 0.5017\n",
      "Validation Accuracy: 83.82%\n",
      "Epoch 22, Loss: 0.4890\n",
      "Validation Accuracy: 83.06%\n",
      "Epoch 23, Loss: 0.4970\n",
      "Validation Accuracy: 81.54%\n",
      "Epoch 24, Loss: 0.5034\n",
      "Validation Accuracy: 84.58%\n",
      "Epoch 25, Loss: 0.5002\n",
      "Validation Accuracy: 82.68%\n",
      "Epoch 26, Loss: 0.5093\n",
      "Validation Accuracy: 81.16%\n",
      "Epoch 27, Loss: 0.5062\n",
      "Validation Accuracy: 79.14%\n",
      "Epoch 28, Loss: 0.5421\n",
      "Validation Accuracy: 82.93%\n",
      "Epoch 29, Loss: 0.5479\n",
      "Validation Accuracy: 79.27%\n",
      "Epoch 30, Loss: 0.5540\n",
      "Validation Accuracy: 79.39%\n",
      "Epoch 31, Loss: 0.5596\n",
      "Validation Accuracy: 81.92%\n",
      "Epoch 32, Loss: 0.5620\n",
      "Validation Accuracy: 81.54%\n",
      "Epoch 33, Loss: 0.5724\n",
      "Validation Accuracy: 80.28%\n",
      "Epoch 34, Loss: 0.5370\n",
      "Validation Accuracy: 79.27%\n",
      "Epoch 35, Loss: 0.5590\n",
      "Validation Accuracy: 74.08%\n",
      "Epoch 36, Loss: 0.5538\n",
      "Validation Accuracy: 78.51%\n",
      "Epoch 37, Loss: 0.5453\n",
      "Validation Accuracy: 78.89%\n",
      "Epoch 38, Loss: 0.5556\n",
      "Validation Accuracy: 76.86%\n",
      "Epoch 39, Loss: 0.5555\n",
      "Validation Accuracy: 82.43%\n",
      "Epoch 40, Loss: 0.5090\n",
      "Validation Accuracy: 81.29%\n",
      "Epoch 41, Loss: 0.5371\n",
      "Validation Accuracy: 76.99%\n",
      "Epoch 42, Loss: 0.4850\n",
      "Validation Accuracy: 75.47%\n",
      "Epoch 43, Loss: 0.4911\n",
      "Validation Accuracy: 78.13%\n",
      "Epoch 44, Loss: 0.4647\n",
      "Validation Accuracy: 82.30%\n",
      "Epoch 45, Loss: 0.4838\n",
      "Validation Accuracy: 81.04%\n",
      "Epoch 46, Loss: 0.4587\n",
      "Validation Accuracy: 84.45%\n",
      "Epoch 47, Loss: 0.4198\n",
      "Validation Accuracy: 77.37%\n",
      "Epoch 48, Loss: 0.4404\n",
      "Validation Accuracy: 86.09%\n",
      "Epoch 49, Loss: 0.4060\n",
      "Validation Accuracy: 83.69%\n",
      "Epoch 50, Loss: 0.4084\n",
      "Validation Accuracy: 83.94%\n",
      "Epoch 51, Loss: 0.3773\n",
      "Validation Accuracy: 86.35%\n",
      "Epoch 52, Loss: 0.3793\n",
      "Validation Accuracy: 87.36%\n",
      "Epoch 53, Loss: 0.3544\n",
      "Validation Accuracy: 86.60%\n",
      "Epoch 54, Loss: 0.3329\n",
      "Validation Accuracy: 88.62%\n",
      "Epoch 55, Loss: 0.3324\n",
      "Validation Accuracy: 88.62%\n",
      "Epoch 56, Loss: 0.3318\n",
      "Validation Accuracy: 89.63%\n",
      "Epoch 57, Loss: 0.3172\n",
      "Validation Accuracy: 87.48%\n",
      "Epoch 58, Loss: 0.3132\n",
      "Validation Accuracy: 90.52%\n",
      "Epoch 59, Loss: 0.3003\n",
      "Validation Accuracy: 89.00%\n",
      "Epoch 60, Loss: 0.3106\n",
      "Validation Accuracy: 89.25%\n",
      "Epoch 61, Loss: 0.2926\n",
      "Validation Accuracy: 90.64%\n",
      "Epoch 62, Loss: 0.3054\n",
      "Validation Accuracy: 90.27%\n",
      "Epoch 63, Loss: 0.2986\n",
      "Validation Accuracy: 90.90%\n",
      "Epoch 64, Loss: 0.2985\n",
      "Validation Accuracy: 88.24%\n",
      "Epoch 65, Loss: 0.2956\n",
      "Validation Accuracy: 87.99%\n",
      "Epoch 66, Loss: 0.3109\n",
      "Validation Accuracy: 88.24%\n",
      "Epoch 67, Loss: 0.3245\n",
      "Validation Accuracy: 89.76%\n",
      "Epoch 68, Loss: 0.3215\n",
      "Validation Accuracy: 87.86%\n",
      "Epoch 69, Loss: 0.3339\n",
      "Validation Accuracy: 86.35%\n",
      "Epoch 70, Loss: 0.3580\n",
      "Validation Accuracy: 86.85%\n",
      "Epoch 71, Loss: 0.3577\n",
      "Validation Accuracy: 82.81%\n",
      "Epoch 72, Loss: 0.3849\n",
      "Validation Accuracy: 86.60%\n",
      "Epoch 73, Loss: 0.3883\n",
      "Validation Accuracy: 85.21%\n",
      "Epoch 74, Loss: 0.3965\n",
      "Validation Accuracy: 82.43%\n",
      "Epoch 75, Loss: 0.3965\n",
      "Validation Accuracy: 74.72%\n",
      "Epoch 76, Loss: 0.4017\n",
      "Validation Accuracy: 73.58%\n",
      "Epoch 77, Loss: 0.4120\n",
      "Validation Accuracy: 84.83%\n",
      "Epoch 78, Loss: 0.4310\n",
      "Validation Accuracy: 83.69%\n",
      "Epoch 79, Loss: 0.4119\n",
      "Validation Accuracy: 68.39%\n",
      "Epoch 80, Loss: 0.4088\n",
      "Validation Accuracy: 81.54%\n",
      "Epoch 81, Loss: 0.4151\n",
      "Validation Accuracy: 80.28%\n",
      "Epoch 82, Loss: 0.4031\n",
      "Validation Accuracy: 83.69%\n",
      "Epoch 83, Loss: 0.3945\n",
      "Validation Accuracy: 84.45%\n",
      "Epoch 84, Loss: 0.3619\n",
      "Validation Accuracy: 87.10%\n",
      "Epoch 85, Loss: 0.3542\n",
      "Validation Accuracy: 82.81%\n",
      "Epoch 86, Loss: 0.3557\n",
      "Validation Accuracy: 82.05%\n",
      "Epoch 87, Loss: 0.3666\n",
      "Validation Accuracy: 88.62%\n",
      "Epoch 88, Loss: 0.3200\n",
      "Validation Accuracy: 84.70%\n",
      "Epoch 89, Loss: 0.3034\n",
      "Validation Accuracy: 85.08%\n",
      "Epoch 90, Loss: 0.3227\n",
      "Validation Accuracy: 87.86%\n",
      "Epoch 91, Loss: 0.3145\n",
      "Validation Accuracy: 88.12%\n",
      "Epoch 92, Loss: 0.2735\n",
      "Validation Accuracy: 88.87%\n",
      "Epoch 93, Loss: 0.2741\n",
      "Validation Accuracy: 90.77%\n",
      "Epoch 94, Loss: 0.2739\n",
      "Validation Accuracy: 89.38%\n",
      "Epoch 95, Loss: 0.2570\n",
      "Validation Accuracy: 90.64%\n",
      "Epoch 96, Loss: 0.2464\n",
      "Validation Accuracy: 89.13%\n",
      "Epoch 97, Loss: 0.2366\n",
      "Validation Accuracy: 91.53%\n",
      "Epoch 98, Loss: 0.2402\n",
      "Validation Accuracy: 92.54%\n",
      "Epoch 99, Loss: 0.2357\n",
      "Validation Accuracy: 91.40%\n",
      "Epoch 100, Loss: 0.2281\n",
      "Validation Accuracy: 91.91%\n",
      "Meilleure précision: 92.54%\n"
     ]
    }
   ],
   "source": [
    "best_acc = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=100)\n",
    "print(f\"Meilleure précision: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a7715-e656-4d55-aab7-251b0bf56e94",
   "metadata": {},
   "source": [
    "#####\n",
    "### Cette partie charge un modèle de réseau de neurones convolutif (CNN) pré-entraîné pour classifier les maladies des feuilles d'arachide à partir d'images\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973764ac-4675-4398-b08f-2397ba3e2d06",
   "metadata": {},
   "source": [
    "### Importation des bibliothèques nécessaires\n",
    "* Cette section importe les modules requis pour la manipulation des images et l'utilisation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b237ddcd-274f-40d6-81fe-8e219a510a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38826498-6b82-4e12-9210-38e10cdd8ebc",
   "metadata": {},
   "source": [
    "### Chargement du modèle entraîné\n",
    "* Définition du chemin du modèle sauvegardé et du nombre de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "946eb993-26b7-4b05-93a7-ac433377c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\abous\\Desktop\\best_model.pth\"\n",
    "num_classes = 6  # Modifier selon le nombre de classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0894bf7-26b9-4615-8420-6d6957fc1c23",
   "metadata": {},
   "source": [
    "### Définition du modèle CNN\n",
    "* Création d'un modèle CNN simple pour la classification des maladies des feuilles d'arachide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2602eb61-04dc-47af-a547-0baf5a31b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):  # Correction de l'initialisation\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self._to_linear = None\n",
    "        self.convs(torch.zeros(1, 3, 128, 128))  # Dummy pass pour calculer la taille\n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.numel()\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b5c87-40b8-4d30-b9ac-06dc82030130",
   "metadata": {},
   "source": [
    "### Chargement du modèle pré-entraîné\n",
    "* Chargement du modèle et mise en mode évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f04207a-a5d1-482d-97e3-981b8622b3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedCNN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImprovedCNN(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26a311-47ad-4070-8255-aff35f0657c3",
   "metadata": {},
   "source": [
    "### Définition des transformations de l'image\n",
    "* Transformation pour adapter les images au format du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "738f82f1-a3e9-419e-aaad-1645c0e624c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8a805-b735-49d0-b5e5-75c6581345f0",
   "metadata": {},
   "source": [
    "### Fonction de prédiction\n",
    "* Prend une image en entrée, applique la transformation et retourne la classe prédite avec la confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd8d8e94-7d61-405f-9873-99fcf5f28901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = torch.softmax(output, dim=1)  # Convertir en probabilités\n",
    "        confidence, predicted = torch.max(probabilities, 1)  # Probabilité max et index associé\n",
    "    \n",
    "    class_names = [\"early_leaf_spot_1\",\"early_rust_1\",\"healthy_leaf_1\",\"late_leaf_spot_1\",\"nutrition_deficiency_1\",\"rust_1\"]  # Modifier selon les classes\n",
    "    return class_names[predicted.item()], confidence.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a767698-e05b-4d6c-bcae-d605861c24e5",
   "metadata": {},
   "source": [
    "### Exemple d'utilisation\n",
    "* Charger une image et obtenir la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec91608c-d23d-490f-9b85-5931b94235f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La plante est classifiée comme : healthy_leaf_1 (Confiance : 92.92%)\n"
     ]
    }
   ],
   "source": [
    "image_path = r'C:\\Users\\abous\\Desktop\\test_22.jpg'  # Modifier avec le chemin de votre image\n",
    "disease, confidence = predict(image_path)\n",
    "print(f\"La plante est classifiée comme : {disease} (Confiance : {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23037a94-c539-4a5e-b64c-5904a4fd4c6f",
   "metadata": {},
   "source": [
    "#####\n",
    "### Cette partie implémente une interface graphique avec customtkinter pour détecter les maladies des feuilles d’arachide à partir d’images puis on crée le transforme en fichier executable (.exe) avec le modele inclus\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57916fe-86ea-4bfa-b307-a5f068f095f1",
   "metadata": {},
   "source": [
    "#####\n",
    "### Lien de telechargement de l'application :\n",
    "https://drive.google.com/file/d/1Famvx92P9CZFCI4opPrR0B72mYP2VrNY/view?usp=sharing\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647296a4-3341-4423-ba2d-eaa73d4371a5",
   "metadata": {},
   "source": [
    "#####\n",
    "### Lien de telechargement du code source de l'application et le modele  :\n",
    "https://drive.google.com/file/d/1OkJ5qlP7QveTDmm9fWL9GgMqf0WcNOdU/view?usp=sharing\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdf610-8bd8-4b2e-8835-27de726b0451",
   "metadata": {},
   "source": [
    "### Vous Pouvez utiliser le dossier \"test\" pour prendre des images et tester l'appliquation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66af818-c55a-4681-861b-709f00d51389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
